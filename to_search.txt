`models/document_chunk.py`
def search_document_chunk(self, question_embedding, top_k, corpus_key: str, threshold: float):
        print(f"we got {corpus_key} with  {threshold} value")
        conn = settings.get_db_connection()
        try:
            cur = conn.cursor()

            # Step 1: Get corpus_id
            cur.execute('SELECT "corpusId" FROM "Corpora" WHERE "corpusKey" = %s;', (corpus_key,))
            corpus_row = cur.fetchone()
            if not corpus_row:
                print("No corpus found for the given key.")
                return {"results": "no corpus found"}

            corpus_id = corpus_row[0]

            # Step 2: Get all document_ids under this corpus
            cur.execute('SELECT "documentId" FROM "Documents" WHERE "corpusId" = %s;', (corpus_id,))
            document_ids = [row[0] for row in cur.fetchall()]
            if not document_ids:
                print("No documents found for the corpus.")
                return {"results": "no documents found"}

            # Step 3: Main vector similarity query with additional filters
            query = f'''
                SELECT * FROM "DocumentChunks"
                WHERE "embeddingData" <=> %s::vector < %s
                AND "documentId" = ANY(%s)
                ORDER BY "embeddingData" <=> %s::vector
                LIMIT %s;
            '''

            cur.execute(query, (question_embedding, threshold, document_ids, question_embedding, top_k))
            rows = cur.fetchall()

            result = []
            if rows:
                columns = [desc[0] for desc in cur.description]
                for row in rows:
                    result.append(dict(zip(columns, row)))
            return result

        except Exception as e:
            print(f"An error occurred in search_document_chunk: {e}")
            return {"results": "no data found"}
        finally:
            if conn:
                conn.close()


`controllers/document_chunk.py`
ef search_document_chunk(question, top_k, model, corpus_key, threshold):
    if not question:
        raise HTTPException(status_code=400, detail="Search question is required")
        
    if not model:
        raise HTTPException(status_code=400, detail="Embedding model is required")
    
    try:
        # Generate embedding for the question
        question_embedding = get_embedding(model, [question])
        if not question_embedding or len(question_embedding) == 0:
            raise HTTPException(status_code=500, detail="Failed to generate embedding for the question")
            
        question_embedding = question_embedding[0]
        
        # Search for relevant chunks
        chunks = documents_data.search_document_chunk(question_embedding, top_k, corpus_key, threshold)
        
        if not chunks or len(chunks) == 0:
            return {"results": ["No relevant information found for your question."]}
        
        # Extract text from chunks for reranking
        re_ranker_data = []
        for chunk in chunks:
            re_ranker_data.append(chunk["chunkText"])
        
        # Rerank the results
        try:
            re_rank_result = re_rank(question, re_ranker_data, top_k=top_k)
        except Exception as e:
            logger.error(f"Reranking failed: {e}")
            # Fall back to original chunks if reranking fails
            context = "\n\n\n".join([chunk["chunkText"] for chunk in chunks[:2]])
        else:
            # Build context from reranked results
            context = ""
            for result in re_rank_result:
                context += result[1] + "\n\n\n"
        
        # Save context for debugging (optional)
        try:
            with open("test.txt", "w", encoding="utf-8") as file:
                file.write(context)
        except Exception as e:
            logger.warning(f"Failed to write context to file: {e}")
        
        # Generate response using LLM
        prompt = f"""
        question: {question}
        You are a helpful assistant, your task is to summarize the given context of information.

        data: {context}

        If the data is not sufficient to provide an answer, just strictly reply with "Not enough context to provide information."
        """
        
        try:
            result = llm_service(prompt, "", "this is a data about some information")
            return {"results": [result], "chunks": re_rank_result}
        except Exception as e:
            logger.error(f"LLM service failed: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to generate response: {str(e)}")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in search_document_chunk: {e}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")